{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "import functools\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import cess_esp\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import difflib\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implements a variety of functions that preprocess in order to facilitate compararisons\n",
    "#Turns all words into lowercase, removes whitespaces and lemantizes\n",
    "#Also defines best match function to match the most accurate matches\n",
    "\n",
    "nltk.download('cess_esp')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#function to tag parts of speech\n",
    "def pos_tagger(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens, lang='spa')\n",
    "    return tagged\n",
    "\n",
    "def autocomplete(word, word_list, n=1):\n",
    "    matches = difflib.get_close_matches(word, word_list, n=n, cutoff=0.6)\n",
    "    return matches[0] if matches else word\n",
    "\n",
    "\n",
    "def preprocess_value_wordlist(value, spell, word_list):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    value = str(value).lower().strip()\n",
    "    words = value.split()\n",
    "    autocompleted_words = [\n",
    "        spell.correction(word) if word not in word_list else word\n",
    "        for word in words\n",
    "    ]\n",
    "    autocompleted_words = [word if word is not None else '' for word in autocompleted_words]\n",
    "    return ' '.join(autocompleted_words)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_value(value):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    value = str(value).lower().strip()\n",
    "    tokens = word_tokenize(value)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    lemmatized_value = \" \".join(lemmatized_tokens)\n",
    "    return lemmatized_value\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.lower().split() if text else []\n",
    "\n",
    "memo = {}\n",
    "\n",
    "def find_best_match(name1, name2, price1, price2, num1, num2, used_indices):\n",
    "    best_score = -1\n",
    "    best_index = -1\n",
    "    best_match = None\n",
    "\n",
    "    memo_key = (name1, tuple(name2), price1, tuple(price2), num1, tuple(num2))\n",
    "\n",
    "    if memo_key in memo:\n",
    "        return memo[memo_key]\n",
    "\n",
    "    tokens1 = set(tokenize(name1))\n",
    "\n",
    "    for idx, nombre2 in enumerate(name2):\n",
    "        if idx in used_indices:\n",
    "            continue\n",
    "\n",
    "        tokens2 = set(tokenize(nombre2))\n",
    "\n",
    "        word_matches = (len(tokens1.intersection(tokens2)))/len(tokens2)\n",
    "        \n",
    "        print(word_matches)\n",
    "\n",
    "        fuzzy_score = fuzz.token_sort_ratio(name1, nombre2)\n",
    "\n",
    "        # Combine word match count and fuzzy score\n",
    "        combined_score = (word_matches * 100 + fuzzy_score)/2\n",
    "\n",
    "        print(combined_score)\n",
    "\n",
    "        if combined_score > best_score:\n",
    "            best_score = combined_score\n",
    "            best_index = idx\n",
    "            best_match = (name1, price1, num1, nombre2, price2[idx], num2[idx] if num2 is not None else None, combined_score, fuzzy_score)\n",
    "\n",
    "    memo[memo_key] = (best_match, best_index)\n",
    "    \n",
    "    return best_match, best_index\n",
    "\n",
    "# Define memoization since its not defined for Rapid Fuzz\n",
    "def memoize(func):\n",
    "    cache = {}\n",
    "    @functools.wraps(func)\n",
    "    def memoized_func(*args):\n",
    "        if args not in cache:\n",
    "            cache[args] = func(*args)\n",
    "        return cache[args]\n",
    "    return memoized_func\n",
    "\n",
    "@memoize\n",
    "def fuzzy_match_score(s1, s2):\n",
    "    return fuzz.token_sort_ratio(s1, s2)\n",
    "\n",
    "def find_best_sku_match(id1, id2):\n",
    "    for id2 in id2:\n",
    "        if id1 in id2 or id2 in id1:\n",
    "            return id2\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def sku_match_score(id1, id2):\n",
    "    if id1 == id2:\n",
    "        return 100  \n",
    "    elif id1 in id2 or id2 in id1:\n",
    "        common_length = min(len(id1), len(id2))\n",
    "        max_length = max(len(id1), len(id2))\n",
    "        return 80 * (common_length / max_length)  \n",
    "    else:\n",
    "        return 0  \n",
    "\n",
    "#Noramlized price difference returns a score for nearness in price\n",
    "\n",
    "def normalize_price_diff(price1, price2):\n",
    "    return 100 * (1 - min(abs(price1 - price2) / price1, 1))\n",
    "\n",
    "\n",
    "def is_within_price_threshold(price1, price2, threshold_percentage):\n",
    "    return np.abs(price1 - price2) / price1 <= threshold_percentage\n",
    "\n",
    "def clean_id(value):\n",
    "    try:\n",
    "        value_str = str(value)\n",
    "        if value_str.endswith('.0'):\n",
    "            value_str = value_str[:-2]\n",
    "        return value_str\n",
    "    except ValueError:\n",
    "        return value  \n",
    "\n",
    "def extract_unique_words(column):\n",
    "    unique_words = set()\n",
    "    for description in column.dropna().unique():\n",
    "        words = description.lower().strip().split()\n",
    "        unique_words.update(words)\n",
    "    return list(unique_words)\n",
    "\n",
    "def apply_parallel(df, func, *args):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = list(tqdm(executor.map(lambda x: func(x, *args), df), total=len(df)))\n",
    "    return futures\n",
    "\n",
    "def remove_numeric_and_ec(text):\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\bec\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code is responsible for defining functions to clean the data, as well as defining the matching logic which will be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reads values from excel and csv depending on requirements can be changed. \n",
    "\n",
    "excel_file_1 = pd.read_excel(\"path to file 1\")\n",
    "excel_file_2 = pd.read_csv(\"path to file 2\")\n",
    "\n",
    "#Resets the index in order to make sure that empty excel cells dont cause a index error\n",
    "\n",
    "excel_file_1 = excel_file_1[~excel_file_1[\"needed column\"].isnull()] \n",
    "excel_file_1.reset_index(inplace=True)\n",
    "\n",
    "database1 = excel_file_1[[\"name\",\"price\",\"id\"]]\n",
    "database1.columns = database1.columns.str.strip().str.replace(' ', '_')\n",
    "\n",
    "database2 = excel_file_2[[\"name\",\"avg_price\",\"id\", \"avg_discount\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reads from the specified files and I specifically reset the index because I was getting an error of having more cells than necessary because the excel files were formated in a bad manner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "# Preprocess data 1 \n",
    "df_1 = database1.apply(lambda x: x.apply(preprocess_value) if x.name in [\"name\", \"id\"] else x)\n",
    "\n",
    "df_1.dropna(subset=['price'], inplace=True)  \n",
    "\n",
    "df_1['name'] = df_1['name'].apply(remove_numeric_and_ec)\n",
    "\n",
    "df_1['price'] = pd.to_numeric(df_1['price'], errors='coerce') \n",
    "\n",
    "dictionary = set(df_1['name'].dropna().str.lower().str.split().sum())\n",
    "\n",
    "print(\"base 1 done\")\n",
    "\n",
    "database2 = database2.rename(columns={\"avg_price\": \"price\", \"avg_discount\": 'discount',})\n",
    "\n",
    "# Preprocess data 2 (database2) including discount column\n",
    "df_2 = database2.apply(lambda x: x.progress_apply(preprocess_value) if x.name in [\"name\",\"id\", \"discount\"] else x)\n",
    "\n",
    "df_2['name'] = df_2['name'].progress_apply(remove_numeric_and_ec)\n",
    "\n",
    "df_2['price'] = pd.to_numeric(df_2['price'], errors='coerce')\n",
    "\n",
    "df_2['discount'] = pd.to_numeric(df_2['discount'], errors='coerce')\n",
    "\n",
    "df_2.dropna(subset=['precio'], inplace=True)\n",
    "\n",
    "print(\"base 2 done\")\n",
    "\n",
    "# Group by id and name in df_2_grouped\n",
    "\n",
    "df_2_grouped = df_2([\"id\", \"name\"], as_index=False).agg(promedio_precio=('price', 'mean'))\n",
    "\n",
    "df_2_grouped = df_2_grouped.rename(columns={'avg_price':'price'})\n",
    "\n",
    "df_2['name'] = df_2_grouped['name'].progress_apply(lambda x: preprocess_value(x))\n",
    "\n",
    "desc_prod_array = df_2_grouped['name'].to_numpy()\n",
    "\n",
    "df_2['price'] = pd.to_numeric(df_2['price'], errors='coerce')  \n",
    "\n",
    "# Apply the cleaning function to the id column in all df\n",
    "df_1['id'] = df_1['id'].apply(clean_id)\n",
    "df_2['id'] = df_2['id'].apply(clean_id)\n",
    "df_2_grouped['id'] = df_2_grouped['id'].apply(clean_id)\n",
    "\n",
    "# Check the results\n",
    "print(df_2[['id']].head())\n",
    "print(df_2_grouped[['id']].head())\n",
    "\n",
    "print(\"df_1 info:\")\n",
    "df_1.info()\n",
    "\n",
    "print(\"df_2 info:\")\n",
    "df_2.info()\n",
    "\n",
    "print(\"df_2_agrupado info:\")\n",
    "df_2_grouped.info()\n",
    "\n",
    "print(\"df_1 head:\")\n",
    "print(df_1.head())\n",
    "\n",
    "print(\"df_2 head:\")\n",
    "print(df_2.head())\n",
    "\n",
    "#Creates arrays in order to work easier and not have to reference which column in the data frames\n",
    "\n",
    "name1_array = df_1['name'].values\n",
    "price1_array = df_1['price'].values\n",
    "numero1_array = df_1['id'].values\n",
    "\n",
    "nombre2_array = df_2_grouped['name'].values\n",
    "precio2_array = df_2_grouped['precio'].values\n",
    "numero2_array = df_2_grouped['id'].values\n",
    "discount2_array = df_2_grouped['discount'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleans the data and groups the values in the second document because the first document has the products we need to find and the second document is like recepits so we can find the matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(df_2_grouped['id'])\n",
    "\n",
    "id_stats = df_2_grouped.groupby('id')['price'].agg(['max', 'min', 'mean', 'std']).reset_index()\n",
    "id_stats.columns = ['id', 'Max Price', 'Min Price', 'Average Price', 'Std Deviation']\n",
    "\n",
    "tokens1 = np.array([set(tokenize(text)) for text in df_1['name']])\n",
    "tokens2 = np.array([set(tokenize(text)) for text in df_2_grouped['name']])\n",
    "\n",
    "df_2_grouped['avg_discount'] = df_2.groupby('id')['discount'].transform('mean')\n",
    "\n",
    "word_matches = np.zeros((len(df_1), len(df_2_grouped)), dtype=int)\n",
    "fuzzy_scores = np.zeros((len(df_1), len(df_2_grouped)), dtype=int)\n",
    "price_scores = np.zeros((len(df_1), len(df_2_grouped)), dtype=int)\n",
    "sku_scores = np.zeros((len(df_1), len(df_2_grouped)), dtype=float)\n",
    "\n",
    "for i in tqdm(range(len(df_1)), desc=\"Processing Rows\", unit=\"row\"):\n",
    "    for j in range(len(df_2_grouped)):\n",
    "        word_matches[i, j] = 100 * len(tokens1[i].intersection(tokens2[j])) / len(tokens1[i])\n",
    "        fuzzy_scores[i, j] = fuzzy_match_score(df_1['name'].iloc[i], df_2_grouped['name'].iloc[j])\n",
    "        price_scores[i, j] = normalize_price_diff(df_1['price'].iloc[i], df_2_grouped['price'].iloc[j])\n",
    "        sku_scores[i, j] = sku_match_score(df_1['id'].iloc[i], df_2_grouped['id'].iloc[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "counts the number of products in the grouped file and creates stats for the products, then gets the matches for each product as scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define threshold and weight values\n",
    "combined_score_threshold = 0\n",
    "price_diff_threshold = 0.1\n",
    "weight_word_matches = 0.45\n",
    "weight_fuzzy_score = 0.50\n",
    "weight_price_score = 0.05\n",
    "weight_sku = 0.0\n",
    "\n",
    "best_matches = []\n",
    "\n",
    "used_indices_1_word_first = set()\n",
    "used_indices_2_word_first = set()\n",
    "used_indices_1_fuzzy_first = set()\n",
    "used_indices_2_fuzzy_first = set()\n",
    "\n",
    "#word first logic\n",
    "for i, row1 in df_1.iterrows():\n",
    "    sku1 = row1['id']\n",
    "    best_match = None\n",
    "    best_combined_score = -1  \n",
    "\n",
    "    top_word_matches_indices = np.argsort(word_matches[i])[::-1][:10]  \n",
    "\n",
    "    for best_index in top_word_matches_indices:\n",
    "        if best_index not in used_indices_2_word_first:\n",
    "            if is_within_price_threshold(df_1['price'][i], df_2_grouped['price'][best_index], price_diff_threshold):\n",
    "                fuzzy_score = fuzzy_scores[i, best_index]\n",
    "                combined_score = (word_matches[i, best_index] * weight_word_matches) + \\\n",
    "                                 (fuzzy_score * weight_fuzzy_score) + \\\n",
    "                                 (price_scores[i, best_index] * weight_price_score) + \\\n",
    "                                 (sku_scores[i, best_index] * weight_sku)\n",
    "\n",
    "                if combined_score >= combined_score_threshold and combined_score > best_combined_score:\n",
    "                    best_combined_score = combined_score\n",
    "                    name1 = df_1['name'][i]\n",
    "                    price1 = df_1['price'][i]\n",
    "                    id1 = df_1['id'][i]\n",
    "                    name2 = df_2_grouped['name'][best_index]\n",
    "                    price2 = df_2_grouped['price'][best_index]\n",
    "                    matched_id = df_2_grouped['id'][best_index]\n",
    "                    repetitions = counter[matched_id]\n",
    "                    avg_descuento = df_2_grouped['avg_discount'][best_index]\n",
    "\n",
    "                    dif_price = np.abs(price1 - price2)\n",
    "\n",
    "                    best_match = (\n",
    "                        name1, price1, id1, name2, price2, matched_id,\n",
    "                        combined_score, fuzzy_score, repetitions,\n",
    "                        dif_price, avg_discount,\n",
    "                        word_matches[i, best_index], price_scores[i, best_index]\n",
    "                    )\n",
    "\n",
    "    # Fuzzy First Logic\n",
    "    top_fuzzy_matches_indices = np.argsort(fuzzy_scores[i])[::-1][:10] \n",
    "\n",
    "    for best_index in top_fuzzy_matches_indices:\n",
    "        if best_index not in used_indices_2_fuzzy_first:\n",
    "            if is_within_price_threshold(df_1['price'][i], df_2_grouped['price'][best_index], price_diff_threshold):\n",
    "                word_score = word_matches[i, best_index]\n",
    "                combined_score = (word_score * weight_word_matches) + \\\n",
    "                                 (fuzzy_scores[i, best_index] * weight_fuzzy_score) + \\\n",
    "                                 (price_scores[i, best_index] * weight_price_score) + \\\n",
    "                                 (sku_scores[i, best_index] * weight_sku)\n",
    "\n",
    "                if combined_score >= combined_score_threshold and combined_score > best_combined_score:\n",
    "                    best_combined_score = combined_score\n",
    "                    name1 = df_1['name'][i]\n",
    "                    price1 = df_1['price'][i]\n",
    "                    id1 = df_1['id'][i]\n",
    "                    name2 = df_2_grouped['name'][best_index]\n",
    "                    price2 = df_2_grouped['price'][best_index]\n",
    "                    matched_id = df_2_grouped['sku'][best_index]\n",
    "                    repetitions = counter[matched_id]\n",
    "                    avg_discount = df_2_grouped['avg_discount'][best_index]\n",
    "\n",
    "                    dif_price = np.abs(price1 - price2)\n",
    "\n",
    "                    best_match = (\n",
    "                        name1, price1, id1, name2, price2, matched_id,\n",
    "                        combined_score, fuzzy_scores[i, best_index], repetitions,\n",
    "                        dif_price, avg_discount,\n",
    "                        word_score, price_scores[i, best_index]\n",
    "                    )\n",
    "\n",
    "    if best_match:\n",
    "        best_matches.append(best_match)\n",
    "\n",
    "        used_indices_1_word_first.add(i)\n",
    "        used_indices_1_fuzzy_first.add(i)\n",
    "        used_indices_2_word_first.add(np.where(df_2_grouped['id'] == best_match[5])[0][0])  \n",
    "        used_indices_2_fuzzy_first.add(np.where(df_2_grouped['id'] == best_match[5])[0][0]) \n",
    "\n",
    "df_best_matches = pd.DataFrame(best_matches, columns=[\n",
    "    'doc 1 name', 'doc 2 price', 'doc 1 id',\n",
    "    'doc 2 name', 'doc 2 price', 'doc 2 id',\n",
    "    'Match Score', 'Fuzzy Score', '# de Repetitions', 'Price difference', 'average discount',\n",
    "    'Word Score', 'Price Score'\n",
    "])\n",
    "\n",
    "df_best_matches['id Match Score'] = weight_sku\n",
    "df_best_matches['Discount Percentage'] = ((df_best_matches['doc 2 price'] - df_best_matches['doc 1 price']) / df_best_matches['doc 1 price']) * 100\n",
    "df_best_matches['trustworthyness'] = np.select(\n",
    "    [df_best_matches['Match Score'] >= 59, df_best_matches['Match Score'] < 59],\n",
    "    [2, 3],\n",
    "    default=np.nan\n",
    ")\n",
    "\n",
    "df_best_matches = df_best_matches[df_best_matches['Match Score'] >= combined_score_threshold]\n",
    "\n",
    "# Output to Excel\n",
    "df_best_matches.to_excel(\"Combined_Best_Matches.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating the scores for each parameter we are using to compare we give it a weight depending on the files and what we consider relevant to the data so its customizable on a case by case basis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
